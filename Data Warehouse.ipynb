{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0f31b8",
   "metadata": {},
   "source": [
    "# DATA WAREHOUSING\n",
    "\n",
    "**What is a Data Warehouse**\n",
    "- A data warehouse is a centralized system that collects and stores large amounts of historical data from various sources, such as sales, marketing, and finance systems, to support business intelligence, reporting, and analytics. \n",
    "- It serves as a single source of truth for an organization, providing a unified and consistent view of data over time to help business users make informed decisions. \n",
    "- Data warehouses differ from operational databases, which support daily operations, by being optimized for analytical queries and long-range historical analysis.  \n",
    "\n",
    "**Key characteristics:**\n",
    "\n",
    "- **Centralized Repository**: It consolidates data from multiple sources into one location. \n",
    "- **Integrated Data**: Data is cleaned, transformed, and standardized into a consistent format for easier analysis. \n",
    "- **Historical Data**: It stores vast amounts of past data, allowing for trend analysis and long-term insights. \n",
    "- **Non-Volatile**: Data in a data warehouse is typically not updated or deleted once stored, preserving historical context. \n",
    "- **Subject-Oriented**: It focuses on specific business subjects, like sales or customer behavior, rather than daily transactions. \n",
    "\n",
    "**Purpose and Use:**\n",
    "\n",
    "- **Business Intelligence (BI)**: Data warehouses are a foundational component of BI, providing the data for dashboards, reports, and analytics tools. \n",
    "- **Informed Decision-Making**: By providing a comprehensive, integrated view of historical and current data, they help businesses understand performance, identify trends, and make smarter decisions. \n",
    "- **Data Mining and Analytics**: It provides a rich dataset for data scientists and analysts to perform data mining, data visualization, and advanced analytics. \n",
    "\n",
    "**How it works (ETL):**\n",
    "\n",
    "- **Extract**: Data is extracted from various source systems (e.g., databases, applications, CRM systems). \n",
    "- **Transform**: The extracted data is cleaned, standardized, and formatted into a common structure. \n",
    "- **Load**: The transformed data is then loaded into the data warehouse for analysis. \n",
    "\n",
    "In essence, a data warehouse acts as a strategic data hub, transforming raw operational data into actionable insights that drive business growth and competitive advantage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae390a",
   "metadata": {},
   "source": [
    "**Key concepts related to data warehousing:**\n",
    "\n",
    "- ``Data mart:`` A smaller subset of a data warehouse focused on a specific area of interest, like sales or marketing, providing a more targeted view for analysis. \n",
    "\n",
    "- ``Operational database vs. Data warehouse:``\n",
    "1. Operational database: Designed for day-to-day transactions, storing current data needed for running the business operations, such as customer orders or inventory levels. \n",
    "2. Data warehouse: Stores historical data from multiple operational databases and other sources, optimized for querying and analysis rather than real-time transactions. \n",
    "- ``OLAP (Online Analytical Processing) vs. OLTP (Online Transaction Processing):``\n",
    "1. OLAP: Used for analytical tasks, querying large datasets with complex calculations to identify trends and patterns. \n",
    "2. OLTP: Used for day-to-day transactions, focusing on fast and efficient data retrieval and updates for operational needs. \n",
    "- ``Fields of application for data warehousing:``\n",
    "1. Marketing analysis: Analyzing customer behavior, campaign performance, and product trends to optimize marketing strategies. \n",
    "2. Financial analysis: Identifying trends, forecasting, and budgeting based on historical financial data. \n",
    "3. Supply chain management: Analyzing inventory levels, supplier performance, and logistics to optimize supply chain efficiency. \n",
    "4. Customer relationship management (CRM): Analyzing customer interactions and feedback to improve customer satisfaction and retention. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5906fb",
   "metadata": {},
   "source": [
    "- Data warehouse features define how these systems support management decisions. Subject-oriented data is organized around key business areas like sales or customers, rather than operational processes. \n",
    "- Integrated data combines information from various sources, resolving inconsistencies to create a unified view. \n",
    "- Time-variant data retains historical information over long periods to enable trend analysis. \n",
    "- Finally, non-volatile data is read-only once loaded, meaning it is not changed or deleted by operational transactions.\n",
    "\n",
    "**Subject-Oriented**\n",
    "\n",
    "Focus: Organizes data around major business subjects, such as customers, products, or sales, rather than day-to-day transactions. \n",
    "Benefit: Allows for comprehensive analysis and strategic decision-making related to specific business domains. \n",
    "\n",
    "**Integrated**\n",
    "\n",
    "Process: Data from disparate operational systems is extracted, transformed, and loaded into a consistent format. \n",
    "Benefit: Resolves naming conflicts, inconsistent data types, and differing units of measure, providing a unified and reliable dataset. \n",
    "\n",
    "**Time-Variant**\n",
    "\n",
    "Data Type: Retains historical data, often for months, years, or longer, unlike operational systems that typically focus on current data. \n",
    "Benefit: Enables trend analysis, historical comparisons, and tracking of business performance over time. \n",
    "\n",
    "**Non-Volatile**\n",
    "\n",
    "Characteristic: Once data is entered into the data warehouse, it is not updated or deleted by day-to-day operational transactions. \n",
    "Benefit: Ensures the stability and consistency of historical data for analysis, as it remains a fixed record of past events. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d955932",
   "metadata": {},
   "source": [
    "## Data WareHouse Architecture\n",
    "- A data warehouse architecture encompasses several layers designed to extract, transform, load, and present data for analytical purposes. The layers you listed represent a comprehensive view of this architecture:\n",
    "\n",
    "**Source System:** This layer consists of the operational databases and various other data sources (e.g., CRM, ERP, flat files, external APIs) where raw transactional and historical data originates.\n",
    "\n",
    "**Source Data Transport Layer**: This layer is responsible for the extraction and movement of data from the source systems to the staging area or directly to the data warehouse. This often involves technologies like ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) tools, data streaming platforms, or change data capture (CDC) mechanisms.\n",
    "\n",
    "**Data Quality Control and Data Profiling Layer**: Before data is integrated into the warehouse, this layer focuses on ensuring its accuracy, completeness, consistency, and validity. Data profiling helps understand the characteristics and quality issues of the source data, while data quality control processes cleanse, standardize, and validate the data to meet predefined quality rules.\n",
    "\n",
    "**Metadata Management Layer**: This layer manages \"data about data.\" It stores information about the data sources, transformations, data models, data lineage, business definitions, and data quality rules. Metadata is crucial for understanding the data, ensuring data governance, and facilitating data discovery for end-users.\n",
    "\n",
    "**Data Integration Layer**: This layer is where data from various sources is combined, transformed, and integrated into a unified format and structure suitable for analytical querying. This often involves applying business rules, resolving inconsistencies, and structuring the data into a dimensional model (e.g., star or snowflake schema) within the data warehouse.\n",
    "\n",
    "**Data Processing Layer**: This layer refers to the core data warehouse engine where the integrated data is stored and optimized for analytical queries. It handles data storage, indexing, and query execution, often utilizing technologies like relational database management systems (RDBMS) or specialized analytical databases.\n",
    "\n",
    "**End User Reporting Layer:** This is the presentation layer where business users interact with the data warehouse to generate reports, perform ad-hoc queries, and conduct data analysis. This layer typically includes business intelligence (BI) tools, reporting dashboards, and data visualization applications that provide insights from the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a597150",
   "metadata": {},
   "source": [
    "## ETL Techniques\n",
    "- ETL (Extract, Transform, Load) is a data integration technique that involves Extracting data from various sources, Transforming it into a usable format by cleansing and structuring it, and then Loading it into a target system, like a data warehouse. \n",
    "- Key ETL techniques include ``batch processing``, for periodic data transfers; ``incremental ETL`` using change data capture to only process new or changed data; ``data cleansing`` to ensure accuracy; and ``data profiling`` to understand data quality before processing.  \n",
    "\n",
    "**Key ETL Techniques**\n",
    "\n",
    "1. **Extraction**: \n",
    "This involves pulling raw data from different sources, such as CRMs, ERPs, or flat files. \n",
    "- ``Full Extraction``: Retrieves all the data from the source system at once. \n",
    "- ``Incremental Extraction (Change Data Capture)``: Uses time or date-based tools to identify and extract only the data that has changed since the last extraction. \n",
    "- ``API Extraction``: Utilizes Application Programming Interfaces to communicate with software and operating systems to extract data. \n",
    "2. **Transformation**: \n",
    "Here, the extracted data is manipulated, cleaned, and standardized. \n",
    "- ``Data Cleansing``: Corrects errors, removes duplicates, and standardizes data formats to improve quality. \n",
    "- ``Data Derivation``: Creates new data fields by applying business rules or calculations to existing data. \n",
    "- ``Data Aggregation``: Summarizes data into a more concise form by grouping and combining related records. \n",
    "- ``Data Integration``: Merges data from different sources into a single, unified dataset. \n",
    "- ``Data Filtering``: Selects only the relevant data based on specific criteria for the target system. \n",
    "3. **Loading**: \n",
    "This is the process of storing the transformed data into a target destination, typically a data warehouse or data lake. \n",
    "- ``Full Load``: Writes the entire dataset into the target system. \n",
    "- ``Incremental Load``: Updates or adds new data to the existing target system based on changes in the source data. \n",
    "\n",
    "**Common Approaches & Considerations**\n",
    "\n",
    "1. ``Staging Area``: A temporary, intermediate storage location for extracted data before it is transformed and loaded. \n",
    "2. ``Batch ET``L: Processes large volumes of data in batches at scheduled intervals. \n",
    "3. ``ETL Tools``: Software like Informatica, Talend, and Microsoft SSIS provides user-friendly interfaces and connectors to facilitate the ETL process. \n",
    "4. ``ETL vs. ELT``: A contrasting approach where data is first loaded into the target system and then transformed within the data storage solution, especially common with cloud data lakes and unstructured data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26509d",
   "metadata": {},
   "source": [
    "## Project setup\n",
    "This project uses:\n",
    "- ``PostgreSQL``: Lightweight server for hosting your SQL database.\n",
    "\n",
    "To setup up your system:\n",
    "1. Download and install PostgreSQL from this [link](https://www.enterprisedb.com/downloads/postgres-postgresql-downloads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929eb8d",
   "metadata": {},
   "source": [
    "## Building the Data Warehouse\n",
    "The aim is to develop a modern data warehouse using PostgreSQL Server to consolidate sales data, enabling analytical reporting.\n",
    "\n",
    "- **Data Sources**: Import data from two source systems (ERP and CRM) provided as CSV files.\n",
    "- **Data Quality**: Cleanse and resolve data quality issues before analysis.\n",
    "- **Integration**: Combine both sources into a user-friendly data model for analytical queries.\n",
    "- **Scope**: Focus on the latest dataset only; historization of data is not required.\n",
    "- **Documentation**: Provide clear data model documentation to support both business stakeholders and analytics teams.\n",
    "\n",
    "There are four different approaches to building a data warehouse:\n",
    "1. InMon\n",
    "2. Kimball\n",
    "3. Data Vault\n",
    "4. Medallion\n",
    "\n",
    "**The Medallion architerture**\n",
    "A medallion architecture serves as a data design blueprint tailored for organizing data within a lake house environment. Its primary aim is to enhance the structure and quality of data gradually as it traverses through successive layers of the architecture, progressing from Bronze to Silver to Gold layers.\n",
    "\n",
    "1. **Bronze layer**\n",
    "The Bronze layer serves as the initial landing ground for all data originating from external source systems. Datasets within this layer mirror the structures of the source system tables in their original state, supplemented by extra metadata columns such as load date/time and process ID. The primary emphasis here is on Change Data Capture, enabling historical archiving of the source data, maintaining data lineage, facilitating audit trails, and allowing for reprocessing if necessary without requiring a fresh read from the source system.\n",
    "\n",
    "2. **Silver layer**\n",
    "The next layer of the warehouse is the Silver layer. Within this layer, data from the Bronze layer undergoes a series of operations to a “just-enough” state (which will be discussed in detail later). This prepares the data in the Silver layer to offer an encompassing “enterprise view” comprising essential business entities, concepts, and transactions.\n",
    "\n",
    "3. **Gold layer**\n",
    "The last layer of the warehouse is the Gold layer. Data within the Gold layer is typically structured into subject area specific databases, primed for consumption. This layer is dedicated to reporting and employs denormalized, read-optimized data models with minimal joins. It serves as the ultimate stage for applying data transformations and quality rules. Commonly, you will observe the integration of Kimball-style star schema based data marts within the Gold Layer of the warehouse.\n",
    "\n",
    "![Alt text](images/medallion.png \"Optional Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7f3c2",
   "metadata": {},
   "source": [
    "## Data Warehouse Structure\n",
    "\n",
    "![Alt text](images/warehouse.png \"Optional Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e38ec6",
   "metadata": {},
   "source": [
    "#### Create the Database, the database schema and the database user\n",
    "- Make sure PostgreSQL is installed in your computer. (Take note of the default username ``postgres`` and the ``password`` set during the installation)\n",
    "- Also ensure the ``psql`` utility is added to the windows path\n",
    "- To create the database and schemas run the following command:\n",
    "```\n",
    "psql -U postgres -d postgress Data-WareHousing\\scripts\\init_db.sql\n",
    "```\n",
    "- This will create the database and the schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60a406",
   "metadata": {},
   "source": [
    "#### Addind data to the database\n",
    "- Create the bronze tables:\n",
    "```\n",
    "psql -U postgres -f scripts/bronze/ddl.sql\n",
    "```\n",
    "- To add data to the database:\n",
    "1. Connect to the database\n",
    "```psql -U postgres -d datawarehouse```\n",
    "2. Copy and paste each command in the ``add_data.txt`` file in the ``scripts`` folder and press ``Enter`` after each line i.e\n",
    "- ```\\copy bronze.crm_cust_info FROM 'datasets\\source_crm\\cust_info.csv' WITH (FORMAT CSV, DELIMITER ',', HEADER);```\n",
    "- ``\\copy bronze.crm_cust_info FROM 'datasets\\source_crm\\cust_info.csv' WITH (FORMAT CSV, DELIMITER ',', HEADER);``\n",
    "- ``\\copy bronze.crm_prd_info FROM 'datasets\\source_crm\\prd_info.csv' WITH (FORMAT CSV, DELIMITER ',', HEADER);``\n",
    "- ``\\copy bronze.crm_sales_details FROM 'datasets\\source_crm\\sales_details.csv' WITH (FORMAT CSV, DELIMITER ',', HEADER);``\n",
    "- ``\\copy bronze.erp_cust_az12 FROM 'datasets\\source_erp\\CUST_AZ12.csv' WITH (FORMAT CSV, DELIMITER ',', HEADER);``\n",
    "- ``\\copy bronze.erp_loc_a101 FROM 'datasets\\source_erp\\LOC_A101.csv' WITH (FORMAT CSV, DELIMITER ',', HEADER);``\n",
    "- ``\\copy bronze.erp_px_cat_g1v2 FROM 'datasets\\source_erp\\PX_CAT_G1V2.csv' WITH (FORMAT CSV, DELIMITER ',', HEADER)``;\n",
    "3. Make sure to enter the correct path to the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9111ecb6",
   "metadata": {},
   "source": [
    "- At this point, the bronze layer of the datawarehouse is built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729c0280",
   "metadata": {},
   "source": [
    "#### Data Flow Bronze layer\n",
    "![](images/bronze.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71739a79",
   "metadata": {},
   "source": [
    "#### ABout the tables\n",
    "- Use the queries in the ``bronze/bronze_overview.sql`` to understand the tables\n",
    "1. Sales table\n",
    "\n",
    "![](images/sales-bronze.PNG)\n",
    "\n",
    "2. Customer table\n",
    "\n",
    "![](images/cust-info-bronze.PNG)\n",
    "\n",
    "3. Product table\n",
    "\n",
    "![](images/prod-info-bronze.PNG)\n",
    "\n",
    "4. Customer birthdays and gender\n",
    "\n",
    "![](images/cust-bday-gender-bronze.PNG)\n",
    "\n",
    "5. Customer location\n",
    "\n",
    "![](images/cust-loc.PNG)\n",
    "\n",
    "6. Product details\n",
    "\n",
    "![](images/prod-details-bronze.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992d3c31",
   "metadata": {},
   "source": [
    "## Silver Layer\n",
    "In the silver layer, we need to transform the data after identifying the quality problems in the data and then coding the transformation script.\n",
    "\n",
    "![](images/silver.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8361e",
   "metadata": {},
   "source": [
    "### Exploring & Understanding the data\n",
    "- We need to understand each table in the bronze layer one by one and check the connection between them (how to join them). \n",
    "- By executing a select statement per table. For example:\n",
    "```\n",
    "SELECT * FROM table_name LIMIT 10;\n",
    "```\n",
    "- For sales details:\n",
    "```\n",
    "SELECT * FROM bronze.crm_sales_details LIMIT 10;\n",
    "```\n",
    "- Use the queries in the ``scripts/bronze/table_structure.sql``\n",
    "- Here is the relationship:\n",
    "![](images/intergration.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c171ba",
   "metadata": {},
   "source": [
    "## Creating DDL for Tables in the Silver Layer\n",
    "- The objective of the silver layer is to have clean and standardized data. \n",
    "- With a full load (truncate & insert) from the bronze layer to the silver layer.\n",
    "\n",
    "**Metadata Columns**\n",
    "\n",
    "- They are extra columns added by the data engineers that do not originate from the source data:\n",
    "\n",
    "1. ``create_date``: The record’s load timestamp\n",
    "2. ``update_date``: The records’s last update time stamp\n",
    "3. ``source_system``: The origin system of the record\n",
    "4. ``file_location``: The file source of the record\n",
    "\n",
    "- To create the DDL for the silver layer, we will just copy the same code that we had for the bronze layer and just replace the keyword ‘bronze.’ with ‘silver.’ and additionally, we will add an extra column for the creation data (and give it. a default value).\n",
    "\n",
    "**Creating the Silver tables**\n",
    "\n",
    "``` psql -U postgres -f silver/ddl.sql```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bf0d2f",
   "metadata": {},
   "source": [
    "#### Data Quality Check\n",
    "- Before inserting the data in the silver tables or doing any transformations, it is very important to check the quality of the data and detect if there are any issues in the data. \n",
    "- If we do not detect the issues, we can not solve them.\n",
    "\n",
    "**Here are some rule of thumb checks to always make**:\n",
    "\n",
    "- Check for duplicates or nulls in the primary keys\n",
    "\n",
    "- Check unwanted space in string values\n",
    "\n",
    "- Check the consistency in low cardinality columns\n",
    "\n",
    "- Checking that dates columns are a real date & not a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d36ea",
   "metadata": {},
   "source": [
    "#### Check for Duplicates\n",
    "\n",
    "- The code used in this section are found in the ``cleaning.sql`` and ``transformation.sql`` files in the ``acripts/silver`` folder.\n",
    "- To check for duplicates and null values\n",
    "```\n",
    "SELECT count(*),\n",
    "cst_id \n",
    "FROM bronze.crm_cust_info \n",
    "GROUP BY (cst_id) \n",
    "HAVING count(cst_id) > 1 or cst_id is NULL;\n",
    "```\n",
    "- The output is:\n",
    "\n",
    "![](images/duplicates.PNG)\n",
    "\n",
    "- So if we check the screenshot below, I selected the rows with 3 duplicates for the same primary key. The strategy here is to take the latest create_date row with the most up-to-date information.\n",
    "\n",
    "```\n",
    "SELECT * FROM bronze.crm_cust_info \n",
    "WHERE cst_id = 29466;\n",
    "```\n",
    "- The output is:\n",
    "\n",
    "![](images/29466.PNG)\n",
    "\n",
    "- To fix this problem, I will use window functions to rank— ``ROW_NUMBER``\n",
    "```\n",
    "SELECT *,\n",
    "ROW_NUMBER() OVER(PARTITION BY cst_id ORDER BY cst_create_date DESC ) as flag_last \n",
    "FROM bronze.crm_cust_info\n",
    "WHERE cst_id = 29466;\n",
    "```\n",
    "\n",
    "![](images/first.PNG)\n",
    "\n",
    "- Then, to remove duplicates, select only the rows with flag_last = 1\n",
    "\n",
    "```\n",
    "SELECT *\n",
    "FROM (\n",
    "    SELECT *,\n",
    "    ROW_NUMBER() OVER(PARTITION BY cst_id ORDER BY cst_create_date DESC ) as flag_last \n",
    "    FROM bronze.crm_cust_info\n",
    "    WHERE cst_id = 29466;\n",
    ")\n",
    "WHERE flag_last = 1;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08593713",
   "metadata": {},
   "source": [
    "#### Check Standardization & Consistency\n",
    "- In low cardinality columns, it is important to check the consistency of the values and deal with Null values according to the conversion in the project. \n",
    "- Check the possible values via the DISTINCT() function.\n",
    "\n",
    "```\n",
    "SELECT DISTINCT(cst_gndr)\n",
    "FROM bronze.crm_cust_info;\n",
    "\n",
    "SELECT DISTINCT(cst_marital_status)\n",
    "FROM bronze.crm_cust_info;\n",
    "```\n",
    "\n",
    "- Then, to make the table more human-readable, I converted the short names to full gender names using the CASE WHEN statement. \n",
    "- We do the same with marital status.\n",
    "\n",
    "#### Checking Dates columns as dates, not strings\n",
    "- Since we defined in the table definition & schema the data type as the date for the column cst_create_date, we do not have to do anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d9695",
   "metadata": {},
   "source": [
    "## Inserting data into the silver layer\n",
    "- After finishing all the transformations, it is time to insert the data into the silver layer\n",
    "\n",
    "```\n",
    "INSERT INTO silver.crm_cust_info(\n",
    "    cst_id,\n",
    "    cast_key,\n",
    "    cst_firstname,\n",
    "    cst_lastname,\n",
    "    cst_marital_status,\n",
    "    cst_gndr,\n",
    "    cst_create_date\n",
    ")\n",
    "SELECT \n",
    "    cst_id,\n",
    "    cast_key,\n",
    "    TRIM(cst_firstname) AS cst_firstname,\n",
    "    TRIM(cst_lastname) AS cst_lastname,\n",
    "    CASE WHEN UPPER(TRIM(cst_marital_status)) = 'S' THEN 'Single'\n",
    "         WHEN UPPER(TRIM(cst_marital_status)) = 'M' THEN 'Married'\n",
    "         ELSE 'n/a'\n",
    "    END cst_marital_status,  -- Normmalize maritial status values to readable format \n",
    "    CASE WHEN UPPER(TRIM(cst_gndr)) = 'F' THEN 'Female'\n",
    "         WHEN UPPER(TRIM(cst_gndr)) = 'M' THEN 'Male'\n",
    "         ELSE 'n/a'\n",
    "    END cst_gndr, -- Normalize gender values to readable format\n",
    "    cst_create_date\n",
    "FROM (\n",
    "    SELECT *, \n",
    "    ROW_NUMBER() OVER(PARTITION BY cst_id ORDER BY cst_create_date DESC) AS flag_last\n",
    "    FROM bronze.crm_cust_info\n",
    ") t \n",
    "WHERE flag_last = 1 AND cst_id is NOT NULL  -- removed duplicated by selecting most recent record per customer \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f904c089",
   "metadata": {},
   "source": [
    "## Gold Layer\n",
    "- At this stage, we will create views so we won’t need to use stored procedures. \n",
    "- To model the data, joins between tables will be created according to the integration model defined in earlier steps.\n",
    "- All these transformations are found in the ``scripts/gold`` folder\n",
    "\n",
    "```\n",
    "SELECT\n",
    "ci.cst_id,\n",
    "ci.cast_key,\n",
    "ci.cst_firstname,\n",
    "ci.cst_lastname,\n",
    "ci.cst_marital_status,\n",
    "ci.cst_gndr,\n",
    "ci.cst_create_date,\n",
    "ca.bdate,\n",
    "ca.gen,\n",
    "la.cntry\n",
    "FROM silver.crm_cust_info ci\n",
    "LEFT JOIN silver.erp_cust_az12 ca \n",
    "ON ci.cast_key = ca.cid \n",
    "LEFT JOIN silver.erp_loc_a101 la\n",
    "ON ci.cast_key = la.cid;\n",
    "```\n",
    "\n",
    "- Another important step in the gold layer is to rename the columns to make them user friendly and still following the naming convention agreed on since the start of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df344d89",
   "metadata": {},
   "source": [
    "#### Dimension Tables\n",
    "- When joining the tables there comes the important question of deciding whether it is a ``dimension`` or a ``fact`` table.\n",
    "\n",
    "- A rule of Thumb is when the columns of a table are descriptive, then it is more likely a dimension.\n",
    "\n",
    "- ``Surrogate Key``: System-generated unique identifier assigned to each record in a table. It is not a business key but it is only used to connect our data model.\n",
    "\n",
    "- To define a surrogate key, we can use a DDL based generation or a more simpler approach using a query with window function (Row_Number).\n",
    "\n",
    "```\n",
    "CREATE VIEW gold.dim_customers AS\n",
    "SELECT\n",
    "    ROW_NUMBER() OVER (ORDER BY cst_id) AS customer_key, -- Surrogate key\n",
    "    ci.cst_id                          AS customer_id,\n",
    "    ci.cast_key                         AS customer_number,\n",
    "    ci.cst_firstname                   AS first_name,\n",
    "    ci.cst_lastname                    AS last_name,\n",
    "    la.cntry                           AS country,\n",
    "    ci.cst_marital_status              AS marital_status,\n",
    "    CASE \n",
    "        WHEN ci.cst_gndr != 'n/a' THEN ci.cst_gndr -- CRM is the primary source for gender\n",
    "        ELSE COALESCE(ca.gen, 'n/a')  \t\t\t   -- Fallback to ERP data / The COALESCE is used to return the first non-null value from a list.\n",
    "    END                                AS gender,\n",
    "    ca.bdate                           AS birthdate,\n",
    "    ci.cst_create_date                 AS create_date\n",
    "FROM silver.crm_cust_info ci\n",
    "LEFT JOIN silver.erp_cust_az12 ca\n",
    "    ON ci.cast_key = ca.cid\n",
    "LEFT JOIN silver.erp_loc_a101 la\n",
    "    ON ci.cast_key = la.cid;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2af9f",
   "metadata": {},
   "source": [
    "## Fact Tables\n",
    "- To recongnize the fact table, look at the columns and they should be showing transactions and events also there can be dates, etc.\n",
    "\n",
    "- To create a data model, it is necessary to connect the fact table with the dimension tables via the surrogate keys instead of IDs by doing joins of the silver layer of the fact table and the gold layer tables of the dimensions.\n",
    "\n",
    "```\n",
    "CREATE VIEW gold.fact_sales AS\n",
    "SELECT\n",
    "    sd.sls_ord_num  AS order_number,\n",
    "    pr.product_key  AS product_key,\n",
    "    cu.customer_key AS customer_key,\n",
    "    sd.sls_order_dt AS order_date,\n",
    "    sd.sls_ship_dt  AS shipping_date,\n",
    "    sd.sls_due_dt   AS due_date,\n",
    "    sd.sls_sales    AS sales_amount,\n",
    "    sd.sls_quantity AS quantity,\n",
    "    sd.sls_price    AS price\n",
    "FROM silver.crm_sales_details sd\n",
    "LEFT JOIN gold.dim_products pr\n",
    "    ON sd.sls_prd_key = pr.product_number\n",
    "LEFT JOIN gold.dim_customers cu\n",
    "    ON sd.sls_cust_id = cu.customer_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5089270f",
   "metadata": {},
   "source": [
    "## Bussiness Intelligence\n",
    "\n",
    "With the gold layer, we can now use the data warehouse to answer various bussiness questions like:\n",
    "- Top ten sales by amount\n",
    "- Top ten customers with the most orders and look at how age, gender and marital status are represented\n",
    "- Top customers with highest purchases\n",
    "- Top products bought\n",
    "- Products that bring in the most money\n",
    "- Purchase patterns over the years i.e top products bought per year\n",
    "- Products that experiemced growth/ decline\n",
    "\n",
    "These queries are found in the ``scripts/gold/gold.sql`` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8c6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import date, datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91680b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a431cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your actual credentials\n",
    "db_user = \"postgres\"\n",
    "db_password = \"postgres\"\n",
    "db_host = \"localhost\"  # or your database host\n",
    "db_port = \"5432\"       # default PostgreSQL port\n",
    "db_name = \"datawarehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b90cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection string\n",
    "connection_string = f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33fa47fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cst_id</th>\n",
       "      <th>cast_key</th>\n",
       "      <th>cst_firstname</th>\n",
       "      <th>cst_lastname</th>\n",
       "      <th>cst_marital_status</th>\n",
       "      <th>cst_gndr</th>\n",
       "      <th>cst_create_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11000.0</td>\n",
       "      <td>AW00011000</td>\n",
       "      <td>Jon</td>\n",
       "      <td>Yang</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>2025-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11001.0</td>\n",
       "      <td>AW00011001</td>\n",
       "      <td>Eugene</td>\n",
       "      <td>Huang</td>\n",
       "      <td>S</td>\n",
       "      <td>M</td>\n",
       "      <td>2025-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11002.0</td>\n",
       "      <td>AW00011002</td>\n",
       "      <td>Ruben</td>\n",
       "      <td>Torres</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>2025-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11003.0</td>\n",
       "      <td>AW00011003</td>\n",
       "      <td>Christy</td>\n",
       "      <td>Zhu</td>\n",
       "      <td>S</td>\n",
       "      <td>F</td>\n",
       "      <td>2025-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11004.0</td>\n",
       "      <td>AW00011004</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>S</td>\n",
       "      <td>F</td>\n",
       "      <td>2025-10-06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cst_id    cast_key cst_firstname cst_lastname cst_marital_status cst_gndr  \\\n",
       "0  11000.0  AW00011000           Jon        Yang                   M        M   \n",
       "1  11001.0  AW00011001        Eugene      Huang                    S        M   \n",
       "2  11002.0  AW00011002         Ruben       Torres                  M        M   \n",
       "3  11003.0  AW00011003       Christy          Zhu                  S        F   \n",
       "4  11004.0  AW00011004     Elizabeth      Johnson                  S        F   \n",
       "\n",
       "  cst_create_date  \n",
       "0      2025-10-06  \n",
       "1      2025-10-06  \n",
       "2      2025-10-06  \n",
       "3      2025-10-06  \n",
       "4      2025-10-06  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define your SQL query\n",
    "products_query = \"SELECT * FROM bronze.crm_cust_info;\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "products_df = pd.read_sql_query(products_query, engine)\n",
    "\n",
    "products_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15bf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SQL query\n",
    "customers_query = \"SELECT * FROM gold.dim_customers;\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "customers_df = pd.read_sql_query(customers_query, engine)\n",
    "\n",
    "customers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15898e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SQL query\n",
    "sales_query = \"SELECT * FROM gold.fact_sales;\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "sales_df = pd.read_sql_query(sales_query, engine)\n",
    "\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c5e72c",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "- Any visualization tool like tableau or power bi can be used to visualize the data.\n",
    "- Here python and matplotlib are used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a36492",
   "metadata": {},
   "source": [
    "#### Top ten most bought products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SQL query\n",
    "products_query = \"\"\"\n",
    "select \n",
    "\tdistinct pa.prd_nm as product,\n",
    "\tpa.prd_line as product_line,\n",
    "\tsa.total_count\n",
    "from gold.top_products as sa\n",
    "inner join silver.crm_prd_info pa\n",
    "on sa.product = pa.prd_key\n",
    "order by total_count desc;\n",
    "\"\"\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "products_df = pd.read_sql_query(products_query, engine)\n",
    "\n",
    "products_df.plot.bar(x='product', y='total_count', title='Top Ten Products by Volume')\n",
    "plt.ylabel('Value') # Add a y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c1408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SQL query\n",
    "products_line_query = \"\"\"\n",
    "select \n",
    "\tproduct_line,\n",
    "\tsum(total_count) as total\n",
    "from (\n",
    "\tselect \n",
    "\tdistinct pa.prd_nm as product,\n",
    "\tpa.prd_line as product_line,\n",
    "\tsa.total_count\n",
    "\tfrom gold.top_products as sa\n",
    "\tinner join silver.crm_prd_info pa\n",
    "\ton sa.product = pa.prd_key\n",
    "\torder by total_count desc\n",
    ")\n",
    "group by (product_line)\n",
    "order by total desc;\n",
    "\"\"\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "products_line_df = pd.read_sql_query(products_line_query, engine)\n",
    "\n",
    "products_line_df.plot.bar(x='product_line', y='total', title='Top Product Lines by Volume')\n",
    "plt.ylabel('Value') # Add a y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b47b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SQL query\n",
    "products_sales_query = \"\"\"\n",
    "select \n",
    "\tdistinct pa.prd_nm as product,\n",
    "\tpa.prd_line as product_line,\n",
    "\tsa.total_count,\n",
    "\tsa.total as sales\n",
    "from gold.top_revenue_products as sa\n",
    "inner join silver.crm_prd_info pa\n",
    "on sa.product = pa.prd_key\n",
    "order by sales desc;\n",
    "\"\"\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "products_sales_df = pd.read_sql_query(products_sales_query, engine)\n",
    "products_sales_df.plot.bar(x='product', y='sales', title='Top Product by Sales')\n",
    "plt.ylabel('Value') # Add a y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9231e",
   "metadata": {},
   "source": [
    "## Online Analytical Processing (OLAP)\n",
    "\n",
    "- OLAP (Online Analytical Processing) operations are the fundamental techniques used to explore, analyze, and summarize multidimensional data stored in a data warehouse. \n",
    "- They allow decision makers to view data from different perspectives for insights. The main OLAP operations are:\n",
    "\n",
    "1. **Roll-up (Consolidation / Aggregation)**\n",
    "Summarizes data by climbing up the hierarchy or reducing dimensions.\n",
    "\n",
    "Example: From daily sales → monthly sales → yearly sales.\n",
    "\n",
    "Or from city level → country level → continent level.\n",
    "\n",
    "2. **Drill-down**\n",
    "Opposite of roll-up. Moves from summarized data to more detailed data.\n",
    "\n",
    "Example: From yearly sales → monthly sales → daily sales.\n",
    "\n",
    "Or from region → country → city → store.\n",
    "\n",
    "3. **Slice**\n",
    "Selects a single dimension value from a cube, creating a sub-cube.\n",
    "\n",
    "Example: Looking at sales for 2024 only, across all products and regions.\n",
    "\n",
    "4. **Dice**\n",
    "Selects data by specifying multiple dimensions and ranges, forming a smaller cube.\n",
    "\n",
    "Example: Sales of Product A and B in the first quarter of 2025 across Kenya and Uganda.\n",
    "\n",
    "5. **Pivot (Rotation**)\n",
    "Reorients the multidimensional view of data to provide an alternative representation.\n",
    "\n",
    "Example: Switching rows and columns to view sales by region vs. sales by product.\n",
    "\n",
    "✅ **Summary Table of OLAP Operations**\n",
    "\n",
    "| Operation | Purpose | Example |\n",
    "|---|---|---|\n",
    "| Roll-up | Summarize / aggregate | Daily → Monthly → Yearly sales |\n",
    "| Drill-down | Get details\tYearly | Yearly → Monthly → Daily sales |\n",
    "| Slice | Fix one dimension | Sales in 2024 |\n",
    "| Dice | Select range of dimensions | Sales of Product A in Q1 2025, Kenya & Uganda |\n",
    "| Pivot | Reorient view | Change rows/columns from \"product vs region\" to \"region vs product\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f5b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SQL query\n",
    "products_sales_query = \"\"\"\n",
    "select \n",
    "\tsa.sls_ord_num as order_number,\n",
    "\tpa.prd_nm as product_name,\n",
    "\tpa.prd_line as product_category,\n",
    "\tsa.sls_order_dt as order_date,\n",
    "\tsa.sls_quantity as quantity,\n",
    "\tsa.sls_sales as total_amount\n",
    "from silver.crm_sales_details sa\n",
    "inner join silver.crm_prd_info pa\n",
    "on sa.sls_prd_key = pa.prd_key;\n",
    "\"\"\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "products_sales_df = pd.read_sql_query(products_sales_query, engine)\n",
    "products_sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241ef055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(int_date: int):\n",
    "    str_date = str(int_date)\n",
    "    year = str_date[:4]\n",
    "    month = str_date[4:6]\n",
    "    day = str_date[6:]\n",
    "    date_obj = date(int(year), int(month), int(day))\n",
    "    return date_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df['order_date'] = products_sales_df['order_date'].apply(format_date)\n",
    "products_sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d17c9",
   "metadata": {},
   "source": [
    "## 1. Roll Up\n",
    "- Here we are going to see the products that were bougt on:\n",
    "    - December 29, 2010\n",
    "    - December of 2010\n",
    "    - In the year 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c106e615",
   "metadata": {},
   "source": [
    "#### Products bought on ``December 29, 2010``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[products_sales_df['order_date'] == date(2010, 12, 29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c6bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[products_sales_df['order_date'] == date(2010, 12, 29)]['product_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac891a",
   "metadata": {},
   "source": [
    "#### Products bought in ``December 2010``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] >= date(2010, 12, 1)) & (products_sales_df['order_date'] <= date(2010, 12, 31))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046e6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] >= date(2010, 12, 1)) & (products_sales_df['order_date'] <= date(2010, 12, 31))]['product_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d392e5",
   "metadata": {},
   "source": [
    "#### Products bought in ``2010``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481bf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] >= date(2010, 1, 1)) & (products_sales_df['order_date'] <= date(2010, 12, 31))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d257568",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] >= date(2010, 1, 1)) & (products_sales_df['order_date'] <= date(2010, 12, 31))]['product_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5d2f8",
   "metadata": {},
   "source": [
    "## 2. Drill Down\n",
    "- Here we are going to see the products that were bougt on:\n",
    "    - In the year 2012\n",
    "    - January of 2012\n",
    "    - January 1, 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6d567",
   "metadata": {},
   "source": [
    "#### Products bought in ``2012``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] >= date(2012, 1, 1)) & (products_sales_df['order_date'] <= date(2012, 12, 31))]['product_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62383094",
   "metadata": {},
   "source": [
    "#### Products bought in ``January 2012``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f91064",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] >= date(2012, 1, 1)) & (products_sales_df['order_date'] <= date(2012, 1, 31))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56495a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] >= date(2012, 1, 1)) & (products_sales_df['order_date'] <= date(2012, 1, 31))]['product_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ff5c1",
   "metadata": {},
   "source": [
    "#### Products bought on ``January 1, 2012``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17acae",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] == date(2012, 1, 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_sales_df[(products_sales_df['order_date'] == date(2012, 1, 1))]['product_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f252d5",
   "metadata": {},
   "source": [
    "## 3. Slice\n",
    "- Here we are going to see all the sales for all products across all regions in 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7891dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SQL query\n",
    "products_regions_query = \"\"\"\n",
    "select \n",
    "\tsa.sls_ord_num as order_number,\n",
    "\tpa.prd_nm as product_name,\n",
    "\tpa.prd_line as product_line,\n",
    "\tsa.sls_order_dt as order_date,\n",
    "\tsa.sls_sales as total_sales,\n",
    "\ter.cntry as country\n",
    "from silver.crm_sales_details sa\n",
    "inner join silver.crm_prd_info pa\n",
    "on sa.sls_prd_key = pa.prd_key\n",
    "inner join silver.crm_cust_info ca\n",
    "on sa.sls_cust_id = ca.cst_id\n",
    "inner join silver.erp_loc_a101 er\n",
    "on ca.cast_key = er.cid\n",
    "where sls_order_dt >= 20130101 and sls_order_dt <= 20131231\n",
    "order by (cntry, prd_line, prd_nm);\n",
    "\"\"\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "products_regions_df = pd.read_sql_query(products_regions_query, engine)\n",
    "products_regions_df['order_date'] = products_regions_df['order_date'].apply(format_date)\n",
    "products_regions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_regions_df['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9602627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Australia\n",
    "products_regions_df[products_regions_df['country'] == 'Australia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867897bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Germany\n",
    "products_regions_df[products_regions_df['country'] == 'Germany']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962617b7",
   "metadata": {},
   "source": [
    "## 4. Dice\n",
    "- Here we look at sales of All Purpose Bike Stands and Touring Tire Toube in the first quarter of 2014 across Australia and Germany."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb67b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your SQL query\n",
    "dice_query = \"\"\"\n",
    "select \n",
    "\tsa.sls_ord_num as order_number,\n",
    "\tpa.prd_nm as product_name,\n",
    "\tpa.prd_line as product_line,\n",
    "\tsa.sls_order_dt as order_date,\n",
    "\tsa.sls_quantity as quamtity,\n",
    "\tsa.sls_sales as total_amount,\n",
    "\ter.cntry as country\n",
    "from silver.crm_sales_details sa\n",
    "inner join silver.crm_prd_info pa\n",
    "on sa.sls_prd_key = pa.prd_key\n",
    "inner join silver.crm_cust_info ca\n",
    "on sa.sls_cust_id = ca.cst_id\n",
    "inner join silver.erp_loc_a101 er\n",
    "on ca.cast_key = er.cid\n",
    "where \n",
    "sls_order_dt >= 20140101 \n",
    "and sls_order_dt <= 20140430\n",
    "and LOWER(prd_nm) in ('all-purpose bike Stand', 'touring tire tube')\n",
    "and lower(cntry) in ('germany', 'australia')\n",
    "order by (cntry, prd_line, prd_nm);\n",
    "\"\"\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "dice_df = pd.read_sql_query(dice_query, engine)\n",
    "dice_df['order_date'] = dice_df['order_date'].apply(format_date)\n",
    "dice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d91a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e751b",
   "metadata": {},
   "source": [
    "## 5. Pivot\n",
    "- We will see the sales by region then pivot to see the sales by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682abb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Sales by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d98ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_query = \"\"\"\n",
    "select \n",
    "\ter.cntry as country,\n",
    "\tsum(sa.sls_sales) as region_sales\n",
    "from silver.crm_sales_details sa\n",
    "inner join silver.crm_prd_info pa\n",
    "on sa.sls_prd_key = pa.prd_key\n",
    "inner join silver.crm_cust_info ca\n",
    "on sa.sls_cust_id = ca.cst_id\n",
    "inner join silver.erp_loc_a101 er\n",
    "on ca.cast_key = er.cid\n",
    "group by cntry\n",
    "order by region_sales;\n",
    "\"\"\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "region_df = pd.read_sql_query(region_query, engine)\n",
    "region_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88613db",
   "metadata": {},
   "source": [
    "#### Sales by product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_query = \"\"\"\n",
    "select \n",
    "\tpa.prd_nm as product,\n",
    "\tsum(sa.sls_sales) as product_sales\n",
    "from silver.crm_sales_details sa\n",
    "inner join silver.crm_prd_info pa\n",
    "on sa.sls_prd_key = pa.prd_key\n",
    "inner join silver.crm_cust_info ca\n",
    "on sa.sls_cust_id = ca.cst_id\n",
    "inner join silver.erp_loc_a101 er\n",
    "on ca.cast_key = er.cid\n",
    "group by prd_nm\n",
    "order by product_sales;\n",
    "\"\"\"\n",
    "\n",
    "# Read data into a pandas DataFrame\n",
    "product_df = pd.read_sql_query(product_query, engine)\n",
    "product_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f7886",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
